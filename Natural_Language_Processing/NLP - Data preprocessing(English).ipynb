{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing(English)\n",
    "1. Cleansing\n",
    "2. Tokenization\n",
    "3. Stopwords\n",
    "4. Stemming\n",
    "5. Lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleansing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def data_processing(text):\n",
    "    text_re = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》]', ' ', text)\n",
    "      \n",
    "    \n",
    "\n",
    "    #Single character removal\n",
    "    text_re = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', text_re)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    text_re = re.sub(r'\\s+', ' ', text_re)\n",
    "    \n",
    "    \n",
    "    return text_re\n",
    "\n",
    "\n",
    "text = \"DF+#$%^ $^&@$%}      a       \"\n",
    "data_processing(text)\n",
    "\n",
    "\"\"\"\n",
    "'DF } '\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DF } '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"DF+#$%^ $^&@$%}      a       \"\n",
    "data_processing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "['The Beluga XL is the successor to the Beluga, or Airbus A300-600ST, which has been in operation since 1995.', 'Its design was adapted from an A330 airliner, with Airbus engineers lowering the flight deck and grafting a huge cargo bay onto the fuselage to create its distinctive shape.Through an upward-opening forward hatch on the \"bubble,\" completed aircraft wings, fuselage sections and other components easily slide in and out.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text_sample = 'The Beluga XL is the successor to the Beluga, or Airbus A300-600ST, which has been in operation since 1995. \\\n",
    "Its design was adapted from an A330 airliner, with Airbus engineers lowering the flight deck and grafting a huge cargo bay onto the fuselage to create its distinctive shape.\\\n",
    "Through an upward-opening forward hatch on the \"bubble,\" completed aircraft wings, fuselage sections and other components easily slide in and out.'\n",
    "sentences = sent_tokenize(text = text_sample)\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)\n",
    "\n",
    "\"\"\"\n",
    "<class 'list'> 2\n",
    "['The Beluga XL is the successor to the Beluga, or Airbus A300-600ST, which has been in operation since 1995.', 'Its design was adapted from an A330 airliner, with Airbus engineers lowering the flight deck and grafting a huge cargo bay onto the fuselage to create its distinctive shape.Through an upward-opening forward hatch on the \"bubble,\" completed aircraft wings, fuselage sections and other components easily slide in and out.']\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 22\n",
      "['The', 'Beluga', 'XL', 'is', 'the', 'successor', 'to', 'the', 'Beluga', ',', 'or', 'Airbus', 'A300-600ST', ',', 'which', 'has', 'been', 'in', 'operation', 'since', '1995', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sentence = 'The Beluga XL is the successor to the Beluga, or Airbus A300-600ST, which has been in operation since 1995.'\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)\n",
    "\n",
    "\"\"\"\n",
    "<class 'list'> 22\n",
    "['The', 'Beluga', 'XL', 'is', 'the', 'successor', 'to', 'the', 'Beluga', ',', 'or', 'Airbus', 'A300-600ST', ',', 'which', 'has', 'been', 'in', 'operation', 'since', '1995', '.']\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stopword 개수 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/park/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "print(\"how many stopword \", len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[0:30])\n",
    "\n",
    "\"\"\"\n",
    "영어 stopword 개수 179\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", \n",
    "'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself'\n",
    ", 'it', \"it's\", 'its', 'itself']\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lov\n",
      "lov\n",
      "lov\n",
      "going\n",
      "went\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nlov\\nlov\\nlov\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "word_list = ['loved', 'loving', 'lovely','going','went']\n",
    "for word in word_list:\n",
    "    print(stemmer.stem(word))\n",
    "    \n",
    "\"\"\"\n",
    "lov\n",
    "lov\n",
    "lov\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse\n",
      "beautiful\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'))\n",
    "print(lemma.lemmatize('beautiful','a'))\n",
    "print(lemma.lemmatize('fanciest','a'))\n",
    "\n",
    "\"\"\"\n",
    "amuse\n",
    "beautiful\n",
    "fancy\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
